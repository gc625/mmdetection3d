2022-10-16 23:16:18,673 - mmdet - INFO - Model:
VoteNet(
  (backbone): Pointformer(
    (SA_modules): ModuleList(
      (0): BasicDownBlock(
        (local_chunk): LocalTransformer(
          (sampler): PointsSampler(
            (samplers): ModuleList(
              (0): DFPSSampler()
            )
          )
          (grouper): QueryAndGroup()
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (global_chunk): GlobalTransformer(
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (linear1): Linear(in_features=64, out_features=128, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=128, out_features=64, bias=True)
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
          )
        )
      )
      (1): BasicDownBlock(
        (local_chunk): LocalTransformer(
          (sampler): PointsSampler(
            (samplers): ModuleList(
              (0): DFPSSampler()
            )
          )
          (grouper): QueryAndGroup()
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (global_chunk): GlobalTransformer(
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=256, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=256, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          )
        )
      )
      (2): BasicDownBlock(
        (local_chunk): LocalTransformer(
          (sampler): PointsSampler(
            (samplers): ModuleList(
              (0): DFPSSampler()
            )
          )
          (grouper): QueryAndGroup()
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (global_chunk): GlobalTransformer(
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (linear1): Linear(in_features=256, out_features=512, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=512, out_features=256, bias=True)
                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
          )
        )
      )
      (3): BasicDownBlock(
        (local_chunk): LocalTransformer(
          (sampler): PointsSampler(
            (samplers): ModuleList(
              (0): DFPSSampler()
            )
          )
          (grouper): QueryAndGroup()
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.0, inplace=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=True)
                (dropout2): Dropout(p=0.0, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (global_chunk): GlobalTransformer(
          (pe): Sequential(
            (0): ConvModule(
              (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): ConvModule(
              (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (chunk): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
              (1): TransformerEncoderLayerPreNorm(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=1024, bias=True)
                (dropout): Dropout(p=0.2, inplace=True)
                (linear2): Linear(in_features=1024, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.2, inplace=True)
                (dropout2): Dropout(p=0.2, inplace=True)
                (activation): ReLU(inplace=True)
              )
            )
          )
          (fc): ConvModule(
            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
        )
      )
    )
    (feature_conv): ConvModule(
      (conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
    )
    (FP_modules): ModuleList(
      (0): PointFPModule(
        (mlps): Sequential(
          (layer0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (layer1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (1): PointFPModule(
        (mlps): Sequential(
          (layer0): ConvModule(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (layer1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
    )
  )
  (bbox_head): VoteHead(
    (objectness_loss): CrossEntropyLoss(avg_non_ignore=False)
    (center_loss): ChamferDistance()
    (dir_res_loss): SmoothL1Loss()
    (dir_class_loss): CrossEntropyLoss(avg_non_ignore=False)
    (size_res_loss): SmoothL1Loss()
    (size_class_loss): CrossEntropyLoss(avg_non_ignore=False)
    (semantic_loss): CrossEntropyLoss(avg_non_ignore=False)
    (vote_module): VoteModule(
      (vote_loss): ChamferDistance()
      (vote_conv): Sequential(
        (0): ConvModule(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): ConvModule(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (conv_out): Conv1d(256, 259, kernel_size=(1,), stride=(1,))
    )
    (vote_aggregation): PointSAModule(
      (groupers): ModuleList(
        (0): QueryAndGroup()
      )
      (mlps): ModuleList(
        (0): Sequential(
          (layer0): ConvModule(
            (conv): Conv2d(259, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (layer1): ConvModule(
            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (layer2): ConvModule(
            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (points_sampler): PointsSampler(
        (samplers): ModuleList(
          (0): DFPSSampler()
        )
      )
    )
    (conv_pred): BaseConvBboxHead(
      (shared_convs): Sequential(
        (layer0): ConvModule(
          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (layer1): ConvModule(
          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (conv_cls): Conv1d(128, 20, kernel_size=(1,), stride=(1,))
      (conv_reg): Conv1d(128, 123, kernel_size=(1,), stride=(1,))
    )
  )